#!/usr/bin/env python
#################################################################################
# Copyright 2018 ROBOTIS CO., LTD.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#################################################################################

# Authors: Gilbert #

import rospy
import os
import json
import numpy as np
import copy
import random
import time
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from src.turtlebot3_dqn.environment_stage_4 import Env

from ..common.settings import ENABLE_VISUAL, ENABLE_STACKING, OBSERVE_STEPS, MODEL_STORE_INTERVAL

from ..common.storagemanager import StorageManager
from ..common.graph import Graph
from ..common.logger import Logger
from ..common.replaybuffer import ReplayBuffer

if ENABLE_VISUAL:
    from ..common.visual import DrlVisual
from ..common import utilities as util

from .algorithms.dqn import DQN
from .algorithms.ddpg import DDPG
from .algorithms.td3 import TD3


EPISODES = 3000

class ReinforceAgent():
    def __init__(self, algorithm, training, load_session="", load_episode=0, train_stage=util.test_stage):
        self.algorithm = algorithm
        self.is_training = int(training)
        self.load_session = load_session
        self.episode = int(load_episode)
        self.train_stage = train_stage

        if (not self.is_training and not self.load_session):
            quit("ERROR no test agent specified")

        self.device = util.check_gpu()
        self.sim_speed = 1

        self.total_steps = 0
        self.observe_steps = OBSERVE_STEPS

        self.model = DDPG(self.device, self.sim_speed)
        self.replay_buffer = ReplayBuffer(self.model.buffer_size)
        self.graph = Graph()

        self.sm = StorageManager(self.algorithm, self.train_stage, self.load_session, self.episode, self.device)

        if self.load_session:
            del self.model
            self.model = self.sm.load_model()
            self.model.device = self.device
            self.sm.load_weights(self.model.networks)
            if self.is_training:
                self.replay_buffer.buffer = self.sm.load_replay_buffer(self.model.buffer_size, os.path.join(self.load_session, 'stage'+str(self.train_stage)+'_latest_buffer.pkl'))
            self.total_steps = self.graph.set_graphdata(self.sm.load_graphdata(), self.episode)
            print(f"global steps: {self.total_steps}")
            print(f"loaded model {self.load_session} (eps {self.episode}): {self.model.get_model_parameters()}")
        else:
            self.sm.new_session_dir()
            self.sm.store_model(self.model)

        self.graph.session_dir = self.sm.session_dir
        self.logger = Logger(self.is_training, self.sm.machine_dir, self.sm.session_dir, self.sm.session, self.model.get_model_parameters(), self.model.get_model_configuration(), str(util.test_stage), self.algorithm, self.episode)
        if ENABLE_VISUAL:
            self.visual = DrlVisual(self.model.state_size, self.model.hidden_size)
            self.model.attach_visual(self.visual)

    def finish_episode(self, step, eps_duration, outcome, dist_traveled, reward_sum, loss_critic, lost_actor):
        print(f"Epi: {self.episode} R: {reward_sum:.2f} outcome: {util.translate_outcome(outcome)} \
                steps: {step} steps_total: {self.total_steps}, time: {eps_duration:.2f}")
        if (self.is_training):
            self.graph.update_data(step, self.total_steps, outcome, reward_sum, loss_critic, lost_actor)
            self.logger.file_log.write(f"{self.episode}, {reward_sum}, {outcome}, {eps_duration}, {step}, {self.total_steps}, \
                                            {self.replay_buffer.get_length()}, {loss_critic / step}, {lost_actor / step}\n")

            if (self.episode % MODEL_STORE_INTERVAL == 0) or (self.episode == 1):
                self.graph.draw_plots(self.episode)
                self.sm.save_session(self.episode, self.model.networks, self.graph.graphdata, self.replay_buffer.buffer)
                self.logger.update_comparison_file(self.episode, self.graph.get_success_count(), self.graph.get_reward_average())
        else:
            self.logger.update_test_results(step, outcome, dist_traveled, eps_duration, 0)
            util.wait_new_goal(self)
    
    
if __name__ == '__main__':
    rospy.init_node('turtlebot3_dqn_stage_test')

    state_size = 28
    action_size = 5

    env = Env(action_size)

    agent = ReinforceAgent("ddpg", "0", "examples/ddpg_0", "8000")

    util.pause_simulation(agent)

    for e in range(0, EPISODES):
        episode_done = False
        step, reward_sum, loss_critic, loss_actor = 0, 0, 0, 0
        action_past = [0.0, 0.0]
        state = env.reset()

        time.sleep(0.5)
        episode_start = time.perf_counter()

        while not episode_done:
            action = agent.model.get_action(state, agent.is_training, step, ENABLE_VISUAL)
            action_current = action

            next_state, reward, episode_done= env.step(action_current, action_past)
            action_past = copy.deepcopy(action_current)
            reward_sum += reward

            if ENABLE_VISUAL:
                agent.visual.update_reward(reward_sum)

            state = copy.deepcopy(next_state)
            step += 1
            time.sleep(agent.model.step_time)

        agent.total_steps += step
        duration = time.perf_counter() - episode_start

        if agent.total_steps >= agent.observe_steps:
            agent.episode += 1
            agent.finish_episode(step, duration, 0, 0, reward_sum, loss_critic, loss_actor)
        else:
            print(f"Observe steps completed: {agent.total_steps}/{agent.observe_steps}")


