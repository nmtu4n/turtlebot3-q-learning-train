#!/usr/bin/env python
#################################################################################
# Copyright 2018 ROBOTIS CO., LTD.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#################################################################################

# Authors: Gilbert #

import rospy
import os
import json
import numpy as np
import random
import time
import sys
from itertools import product
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque
from std_msgs.msg import Float32MultiArray
from src.turtlebot3_dqn.environment_stage_q import Env

EPISODES = 3000
MAX_STEPS_PER_EPISODE = 500

ACTION_NUMBER = 4

ALPHA = 0.5
GAMMA = 0.9

STATE_SPACE_IND_MAX = 1872 - 1
STATE_SPACE_IND_MIN = 1 - 1
ACTIONS_IND_MAX = 3
ACTIONS_IND_MIN = 0


class ReinforceAgent():
    def __init__(self, state_size, action_size):
        self.pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
        self.dirPath = os.path.dirname(os.path.realpath(__file__))
        self.dirPath = self.dirPath.replace('turtlebot3_dqn/nodes', 'turtlebot3_dqn/save_model/q_table/')
        self.dirFile = self.dirPath + "Qtable" + ".csv"
        self.result = Float32MultiArray()

        self.load_model = False
        self.state_size = state_size
        self.action_size = action_size
        self.episode_step = 6000
        
        self.epsilon = 1.0
        self.epsilon_decay = 0.99
        self.epsilon_min = 0.05

        self.actions = self.create_actions()


        if self.load_model:
            self.q_table = self.read_q_table(self.dirFile)
        else:
            self.state_space = self.create_state_space()

            self.q_table = self.create_q_table(len(self.state_space), len(self.actions))

    def create_actions(self):
        actions = np.array([0, 1, 2, 3])
        return actions
    
    def create_state_space(self):
        x1 = set((0,1,2))
        x2 = set((0,1,2))
        x3 = set((0,1,2,3))
        x4 = set((0,1,2,3))
        x5 = set ((0,1,2,3,4,5,6,7,8,9,10,11,12))
        state_space = set(product(x1,x2,x3,x4,x5))

        return np.array(list(state_space))

    def read_q_table(self, path):
        return np.genfromtxt(path, delimiter=' , ')
    
    def create_q_table(self, n_states, n_actions):
        return np.zeros((n_states, n_actions))

    def update_q_table(self, state_ind, prev_state_ind, action, reward):
        if(
            STATE_SPACE_IND_MIN <= state_ind <= STATE_SPACE_IND_MAX and
            STATE_SPACE_IND_MIN <= prev_state_ind <= STATE_SPACE_IND_MAX
        ):

            self.q_table[prev_state_ind, action] = (1 - ALPHA) * self.q_table[prev_state_ind, action]  + ALPHA * (reward + GAMMA * max(self.q_table[state_ind, :])) 
        else:
            print('update_q_table => INVALID STATE INDEX')
    
    def save_q_table(self):
        np.savetxt(self.dirFile, self.q_table, delimiter= ' , ')

    def get_best_action(self, state_ind):
        if STATE_SPACE_IND_MIN <= state_ind <= STATE_SPACE_IND_MAX:
            a_ind = np.argmax(self.q_table[state_ind,:])
            action = self.actions[a_ind]

        else:

            action = self.get_random_action()
        
        return action
    
    def get_random_action(self):
        index = np.random.randint(len(self.actions))
        return self.actions[index]

    def epsilo_greedy_exploration(self, state_ind, epsilon):
        if np.random.uniform() > epsilon and STATE_SPACE_IND_MIN <= state_ind <= STATE_SPACE_IND_MAX:
            action = self.get_best_action(state_ind)

        else:
            action = self.get_random_action()

        return action

if __name__ == '__main__':
    rospy.init_node('turtlebot3_q_train')
    pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
    pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)
    result = Float32MultiArray()
    get_action = Float32MultiArray()

    state_size = 26
    action_size = 5

    agent = ReinforceAgent(state_size, action_size)

    env = Env(action_size, agent.state_space)

    scores, episodes = [], []
    global_step = 0
    start_time = time.time()

    param_keys = ['epsilon']
    param_values = [agent.epsilon]
    param_dictionary = dict(zip(param_keys, param_values))

    for e in range(0, EPISODES):
        done = False
        (state_index, x1, x2, x3, x4, x5), done = env.reset()
        total_episode_reward = 0

        for step in range(MAX_STEPS_PER_EPISODE):
            action = agent.epsilo_greedy_exploration(state_index, agent.epsilon)
            
            (next_state_index, x1, x2, x3, x4, x5), reward, done = env.step(action)

            agent.update_q_table(next_state_index, state_index, action, reward)
            
            state_index = next_state_index

            total_episode_reward = total_episode_reward + reward

            if step >= 500:
                rospy.loginfo("Time out!!")
                done = True

            if e % 50 == 0:
                agent.save_q_table()
                with open(agent.dirPath + str(e) + '.json', 'w') as outfile:
                    json.dump(param_dictionary, outfile)

            if done:
                result.data = [total_episode_reward, action]
                pub_result.publish(result)
                scores.append(total_episode_reward)
                episodes.append(e)
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)

                rospy.loginfo('Ep: %d score: %.2f epsilon: %.2f time: %d:%02d:%02d',
                              e, total_episode_reward, agent.epsilon, h, m, s)
                param_keys = ['epsilon']
                param_values = [agent.epsilon]
                param_dictionary = dict(zip(param_keys, param_values))
                break


        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay
